<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Kiminodatchi</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      margin: 30px;
    }
    #chat {
      border: 1.5px solid #ccc;
      padding: 10px;
      height: 100px;
      overflow-y: scroll;
      margin-bottom: 20px;
    }
    #transcript {
      border: 1.5px solid #ccc;
      padding: 10px;
      height: 300px;
      overflow-y: scroll;
      margin-bottom: 20px;
    }    
    button {
      font-size: 18px;
      padding: 10px 20px;
      margin-right: 10px;
    }
    #micStatus {
      font-weight: bold;
      margin-bottom: 10px;
      display: inline-block;
    }
    .green { color: green; }
    .red { color: red; }
    .gray { color: gray; }
    canvas {
      border: 1px solid #ccc;
      margin-bottom: 20px;
    }
    h3 {
      margin-top: 40px;
    }
  </style>
</head>
<body>
  <h1>Kiminodatchi</h1>
  <div id="status">
    <span id="micStatus" class="gray">üé§ Awaiting permission...</span>
  </div>

  <div id="chat"></div>
  <canvas id="waveform" width="800" height="50"></canvas>
  <button id="startBtn">üéôÔ∏è Start Talking</button>
  <button id="stopListeningBtn" style="display: none;">‚õî Stop Listening</button>
  <button id="stopBtn" style="display: none;">‚èπÔ∏è Stop Conversation</button>

  <h3>üìù Chat Transcript</h3>
  <div id="transcript"></div>

  <script>
    const startBtn = document.getElementById("startBtn");
    const stopBtn = document.getElementById("stopBtn");
    const stopListeningBtn = document.getElementById("stopListeningBtn");
    const chatDiv = document.getElementById("chat");
    const micStatus = document.getElementById("micStatus");
    const transcriptDiv = document.getElementById("transcript");

    let silenceTimer = null;
    let sharedStream = null;
    let conversationActive = false;
    let transcript = [];

    let socket = null;
    let processor = null;
    let source = null;

    function updateMicStatus(state) {
      micStatus.className = '';
      if (state === 'granted') {
        micStatus.textContent = 'üé§ Microphone ready';
        micStatus.classList.add('green');
      } else if (state === 'denied') {
        micStatus.textContent = '‚ùå Microphone access denied';
        micStatus.classList.add('red');
      } else {
        micStatus.textContent = 'üé§ Awaiting permission...';
        micStatus.classList.add('gray');
      }
    }

    async function getOrRequestStream() {
      if (
        !sharedStream ||
        sharedStream.getAudioTracks().length === 0 ||
        sharedStream.getAudioTracks().every(track => track.readyState === "ended")
      ) {
        try {
          sharedStream = await navigator.mediaDevices.getUserMedia({ audio: true });
          updateMicStatus('granted');
        } catch (err) {
          updateMicStatus('denied');
          alert("Microphone access is required.");
          console.error("Mic access error:", err);
          return null;
        }
      }
      return sharedStream;
    }

    function appendChat(text) {
      const p = document.createElement("p");
      p.textContent = text;
      chatDiv.appendChild(p);
      chatDiv.scrollTop = chatDiv.scrollHeight;
    }

    function appendTranscript(speakerOrText, maybeText) {  
      let line; 
      if (maybeText === undefined) {  
        line = speakerOrText;  
      } else {  
        line = `${speakerOrText}: ${maybeText}`;  
      }
      transcript.push(line);
      const p = document.createElement("p");
      p.textContent = line;
      transcriptDiv.appendChild(p);
      transcriptDiv.scrollTop = transcriptDiv.scrollHeight;
    }    

    async function startTalking() {
      const stream = await getOrRequestStream();
      if (!stream) return;

      startBtn.disabled = true;
      stopListeningBtn.style.display = "inline-block";
      stopBtn.style.display = "inline-block";
      conversationActive = true;
      appendChat("üî¥ Listening...");

      socket = new WebSocket("ws://localhost:8000/ws/audio");
      socket.binaryType = "arraybuffer";

      socket.onmessage = (event) => {
        if (event.data instanceof ArrayBuffer) {
          const audioBlob = new Blob([event.data], { type: "audio/mpeg" });
          const audioUrl = URL.createObjectURL(audioBlob);
          const audio = new Audio(audioUrl);
          audio.play();
          appendChat("üü¢ System responded with audio.");
          audio.onended = () => {
            if (conversationActive) {
              setTimeout(() => {
                startTalking();
              }, 300);
            }
          };
        } else {
          appendTranscript(event.data);
        }
      };

      socket.onclose = () => {
        appendChat("üîÅ Connection closed.");
        if (!conversationActive) {
          startBtn.disabled = false;
          stopBtn.style.display = "none";
          stopListeningBtn.style.display = "none";
        }
      };

      const audioContext = new (window.AudioContext || window.webkitAudioContext)({ sampleRate: 16000 });
      await audioContext.resume();

      source = audioContext.createMediaStreamSource(stream);
      processor = audioContext.createScriptProcessor(4096, 1, 1);
      const analyser = audioContext.createAnalyser();
      analyser.fftSize = 1024;
      const bufferLength = analyser.frequencyBinCount;
      const dataArray = new Uint8Array(bufferLength);

      source.connect(analyser);
      source.connect(processor);
      processor.connect(audioContext.destination);

      processor.onaudioprocess = function (event) {
        const input = event.inputBuffer.getChannelData(0);
        const int16Buffer = new Int16Array(input.length);
        let silent = true;

        for (let i = 0; i < input.length; i++) {
          int16Buffer[i] = input[i] * 32767;
          if (Math.abs(input[i]) > 0.01) silent = false;
        }

        if (silent) {
          if (!silenceTimer) {
            silenceTimer = setTimeout(() => {
              socket.send(new Uint8Array([]));
              processor.disconnect();
              source.disconnect();
              appendChat("üõë Stopped recording (1.5s silence)");
              stopListeningBtn.style.display = "none";
            }, 1500);
          }
        } else {
          if (silenceTimer) {
            clearTimeout(silenceTimer);
            silenceTimer = null;
          }
          if (socket.readyState === WebSocket.OPEN) {
            socket.send(int16Buffer.buffer);
          }
        }
      };

      const canvas = document.getElementById("waveform");
      const canvasCtx = canvas.getContext("2d");
      function drawWaveform() {
        requestAnimationFrame(drawWaveform);
        analyser.getByteTimeDomainData(dataArray);
        canvasCtx.fillStyle = "#fff";
        canvasCtx.fillRect(0, 0, canvas.width, canvas.height);
        canvasCtx.lineWidth = 2;
        canvasCtx.strokeStyle = "#007bff";
        canvasCtx.beginPath();
        const sliceWidth = canvas.width / bufferLength;
        let x = 0;
        for (let i = 0; i < bufferLength; i++) {
          const v = dataArray[i] / 128.0;
          const y = v * canvas.height / 2;
          i === 0 ? canvasCtx.moveTo(x, y) : canvasCtx.lineTo(x, y);
          x += sliceWidth;
        }
        canvasCtx.lineTo(canvas.width, canvas.height / 2);
        canvasCtx.stroke();
      }
      drawWaveform();
    }

    function stopListeningManually() {
      if (silenceTimer) {
        clearTimeout(silenceTimer);
        silenceTimer = null;
      }
      if (socket && socket.readyState === WebSocket.OPEN) {
        socket.send(new Uint8Array([]));
      }
      if (processor) processor.disconnect();
      if (source) source.disconnect();
      appendChat("üõë Stopped recording (manual interrupt)");
      stopListeningBtn.style.display = "none";
    }

    function stopConversation() {
      conversationActive = false;
      startBtn.disabled = false;
      stopBtn.style.display = "none";
      stopListeningBtn.style.display = "none";
      appendChat("‚èπÔ∏è Conversation stopped.");
      appendTranscript("System", "[Conversation ended]");
    }

    startBtn.onclick = startTalking;
    stopBtn.onclick = stopConversation;
    stopListeningBtn.onclick = stopListeningManually;

    window.addEventListener("load", async () => {
      updateMicStatus('awaiting');
      await getOrRequestStream();
    });

    const evtSource = new EventSource("/stream");
    evtSource.onmessage = (event) => {
      appendTranscript(event.data);
    };
  </script>
</body>
</html>
